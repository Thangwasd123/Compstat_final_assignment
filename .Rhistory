formula_str <- paste("price_class ~", paste(predictors, collapse = " + "))
# Fit logistic regression with error handling
model <- tryCatch({
glm(as.formula(formula_str),
data = train_fold,
family = binomial(link = "logit"),
control = glm.control(maxit = 50))  # Limit iterations
}, warning = function(w) {
return(NULL)  # Return NULL on warning
}, error = function(e) {
return(NULL)  # Return NULL on error
})
# If model failed, use mean prediction
if (is.null(model)) {
predictions[i] <- mean(train_fold$price_class)
} else {
predictions[i] <- predict(model, newdata = test_fold, type = "response")
}
}
# Convert probabilities to binary predictions
predicted_class <- ifelse(predictions >= threshold, 1, 0)
cv_error <- mean(predicted_class != actuals)
# Confusion matrix
TP <- sum(predicted_class == 1 & actuals == 1)
TN <- sum(predicted_class == 0 & actuals == 0)
FP <- sum(predicted_class == 1 & actuals == 0)
FN <- sum(predicted_class == 0 & actuals == 1)
return(list(
cv_error = cv_error,
accuracy = 1 - cv_error,
predictions = predictions,
predicted_class = predicted_class,
confusion = c(TP = TP, TN = TN, FP = FP, FN = FN)
))
}
# Loop through each model combination
cat("Starting LOOCV for", length(all_models), "models...\n\n")
for (i in 1:length(all_models)) {
predictors <- all_models[[i]]
cat("Model", i, "of", length(all_models), "- Predictors:",
paste(predictors, collapse = ", "), "\n")
# Perform manual LOOCV
loocv_results <- manual_loocv(predictors, trimmed_df)
# Store results
model_results[i, "model_id"] <- i
model_results[i, "predictors"] <- paste(predictors, collapse = ", ")
model_results[i, "num_predictors"] <- length(predictors)
model_results[i, "cv_error"] <- loocv_results$cv_error
}
# Sort by CV Error (lower is better)
model_results <- model_results[order(model_results$cv_error), ]
# Display top 10 models
cat("\n===========================================\n")
cat("Top 10 Models by CV Error (Lower is Better):\n")
cat("===========================================\n")
print(head(model_results, 10))
# Get the best model
best_model_predictors <- all_models[[model_results$model_id[1]]]
cat("\n===========================================\n")
cat("Best Model:\n")
cat("===========================================\n")
cat("Predictors:", paste(best_model_predictors, collapse = ", "), "\n")
cat("CV Error:", model_results$cv_error[1], "\n")
# Basic ggplot histogram
ggplot(model_results, aes(x = cv_error)) +
geom_histogram(bins = 15, fill = "steelblue", color = "black", alpha = 0.7) +
geom_vline(aes(xintercept = min(cv_error)),
color = "red", linetype = "dashed", linewidth = 1) +
annotate("text", x = min(model_results$cv_error), y = Inf,
label = paste0("Best model\n(CV Error = ", round(min(model_results$cv_error), 6), ")"),
hjust = -0.1, vjust = 1.5, color = "red", size = 4) +
labs(title = "Distribution of LOOCV Classification Errors Across All Models",
x = "Cross-Validated Classification Error (MSE)",
y = "Frequency") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"))
model_results <- data.frame(
model_id = integer(),
predictors = character(),
num_predictors = integer(),
accuracy = numeric(),
stringsAsFactors = FALSE
)
# Manual LOOCV function with classification accuracy
manual_loocv <- function(predictors, data, threshold = 0.5) {
n <- nrow(data)
predictions <- numeric(n)
actuals <- data$price_class
# Loop through each observation
for (i in 1:n) {
train_fold <- data[-i, ]
test_fold <- data[i, ]
formula_str <- paste("price_class ~", paste(predictors, collapse = " + "))
# Fit logistic regression
model <- glm(as.formula(formula_str),
data = train_fold,
family = binomial(link = "logit"))
# Predict on the left-out observation
predictions[i] <- predict(model, newdata = test_fold, type = "response")
}
# Convert probabilities to binary predictions
predicted_class <- ifelse(predictions >= threshold, 1, 0)
# Confusion matrix components
TP <- sum(predicted_class == 1 & actuals == 1)  # True Positive
TN <- sum(predicted_class == 0 & actuals == 0)  # True Negative
FP <- sum(predicted_class == 1 & actuals == 0)  # False Positive
FN <- sum(predicted_class == 0 & actuals == 1)  # False Negative
# Calculate accuracy as (TP + TN) / Total
accuracy <- (TP + TN) / n
return(list(
accuracy = accuracy,
cv_error = 1 - accuracy,
predictions = predictions,
predicted_class = predicted_class,
confusion = c(TP = TP, TN = TN, FP = FP, FN = FN)
))
}
# Loop through each model combination
cat("Starting LOOCV for", length(all_models), "models...\n\n")
for (i in 1:length(all_models)) {
predictors <- all_models[[i]]
cat("Model", i, "of", length(all_models), "- Predictors:",
paste(predictors, collapse = ", "), "\n")
# Perform manual LOOCV
loocv_results <- manual_loocv(predictors, trimmed_df)
# Store results
model_results[i, "model_id"] <- i
model_results[i, "predictors"] <- paste(predictors, collapse = ", ")
model_results[i, "num_predictors"] <- length(predictors)
model_results[i, "accuracy"] <- loocv_results$accuracy  # CHANGED: cv_error → accuracy
}
# Sort by Accuracy (HIGHER is better)
model_results <- model_results[order(-model_results$accuracy), ]  # CHANGED: added minus sign, cv_error → accuracy
# Display top 10 models
cat("\n===========================================\n")
cat("Top 10 Models by Accuracy (Higher is Better):\n")  # CHANGED: wording
cat("===========================================\n")
print(head(model_results, 10))
# Get the best model
best_model_predictors <- all_models[[model_results$model_id[1]]]
cat("\n===========================================\n")
cat("Best Model:\n")
cat("===========================================\n")
cat("Predictors:", paste(best_model_predictors, collapse = ", "), "\n")
cat("Accuracy:", model_results$accuracy[1], "\n")  # CHANGED: cv_error → accuracy
library(ggplot2)
# Histogram of accuracy scores
ggplot(model_results, aes(x = accuracy)) +
geom_histogram(bins = 15, fill = "steelblue", color = "black", alpha = 0.7) +
geom_vline(aes(xintercept = max(accuracy)),
color = "red", linetype = "dashed", linewidth = 1) +
annotate("text", x = max(model_results$accuracy), y = Inf,
label = paste0("Best model\n(Accuracy = ", round(max(model_results$accuracy), 4), ")"),
hjust = -0.1, vjust = 1.5, color = "red", size = 4) +
labs(title = "Distribution of LOOCV Classification Accuracy Across All Models",
x = "Cross-Validated Classification Accuracy",
y = "Frequency") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"))
model <- glm(price_class ~ sq_mt_built,
data = train,
family = binomial)
# Fit the best model
model <- glm(price_class ~ sq_mt_built,
data = train,
family = binomial)
# Predict on test set
model.prob <- predict(model, test, type = "response")
# Convert probabilities to binary predictions using 0.5 threshold
predicted_class <- ifelse(model.prob >= 0.5, 1, 0)
actual <- test$price_class
# Calculate accuracy as (TP + TN) / Total
TP <- sum(predicted_class == 1 & actual == 1)
TN <- sum(predicted_class == 0 & actual == 0)
FP <- sum(predicted_class == 1 & actual == 0)
FN <- sum(predicted_class == 0 & actual == 1)
test_accuracy <- (TP + TN) / length(actual)
# Print results
cat("\n=================================\n")
cat("Test Set Performance:\n")
cat("=================================\n")
cat("Test Accuracy:", round(test_accuracy, 4), "\n")
cat("Confusion Matrix:\n")
cat("  TP:", TP, "  TN:", TN, "\n")
cat("  FP:", FP, "  FN:", FN, "\n")
# Summary of the model
cat("\n=================================\n")
cat("Model Summary:\n")
cat("=================================\n")
summary(model)
double_loocv <- function(data, all_models, threshold = 0.5) {
n <- nrow(data)
outer_predictions <- numeric(n)
outer_actuals <- data$price_class
selected_models <- character(n)
cat("Starting Double LOOCV with", n, "outer iterations...\n\n")
# OUTER LOOP: Leave one observation out
for (i in 1:n) {
if (i %% 100 == 0) cat("Outer iteration", i, "of", n, "\n")
# Split data
outer_train <- data[-i, ]
outer_test <- data[i, ]
# INNER LOOP: Model selection using LOOCV on outer_train
inner_results <- data.frame(
model_id = integer(),
accuracy = numeric(),
stringsAsFactors = FALSE
)
for (j in 1:length(all_models)) {
predictors <- all_models[[j]]
# Perform LOOCV on outer_train for this model
inner_accuracy <- manual_loocv(predictors, outer_train, threshold)$accuracy
inner_results[j, "model_id"] <- j
inner_results[j, "accuracy"] <- inner_accuracy
}
# Select best model from inner LOOCV (highest accuracy)
best_model_idx <- which.max(inner_results$accuracy)
best_predictors <- all_models[[best_model_idx]]
selected_models[i] <- paste(best_predictors, collapse = ", ")
# Refit best model on full outer_train
formula_str <- paste("price_class ~", paste(best_predictors, collapse = " + "))
final_model <- glm(as.formula(formula_str),
data = outer_train,
family = binomial(link = "logit"))
# Predict on outer_test (the single left-out observation)
outer_predictions[i] <- predict(final_model, newdata = outer_test, type = "response")
}
# Convert probabilities to binary predictions
predicted_class <- ifelse(outer_predictions >= threshold, 1, 0)
# Calculate confusion matrix components
TP <- sum(predicted_class == 1 & outer_actuals == 1)
TN <- sum(predicted_class == 0 & outer_actuals == 0)
FP <- sum(predicted_class == 1 & outer_actuals == 0)
FN <- sum(predicted_class == 0 & outer_actuals == 1)
# Calculate final accuracy
final_accuracy <- (TP + TN) / n
return(list(
accuracy = final_accuracy,
cv_error = 1 - final_accuracy,
predictions = outer_predictions,
predicted_class = predicted_class,
selected_models = selected_models,
confusion = c(TP = TP, TN = TN, FP = FP, FN = FN)
))
}
# Run double LOOCV
cat("\n===========================================\n")
cat("Running Double/Nested LOOCV...\n")
cat("===========================================\n")
double_loocv_results <- double_loocv(trimmed_df, all_models)
cat("\n===========================================\n")
cat("Double LOOCV Results:\n")
cat("===========================================\n")
cat("Final Accuracy:", round(double_loocv_results$accuracy, 4), "\n")
cat("Final CV Error:", round(double_loocv_results$cv_error, 4), "\n")
cat("\nConfusion Matrix:\n")
print(double_loocv_results$confusion)
# See which models were most frequently selected
cat("\n===========================================\n")
cat("Most frequently selected models:\n")
cat("===========================================\n")
print(head(sort(table(double_loocv_results$selected_models), decreasing = TRUE), 10))
print(double_loocv_results$confusion)
model_results$errors <- 1 - model_results$accuracy
model_results
# Histogram of accuracy scores
ggplot(model_results, aes(x = errors)) +
geom_histogram(bins = 15, fill = "steelblue", color = "black", alpha = 0.7) +
geom_vline(aes(xintercept = max(errors)),
color = "red", linetype = "dashed", linewidth = 1) +
annotate("text", x = min(model_results$accuracy), y = Inf,
label = paste0("Lowest model\n(Error = ", round(max(model_results$errors), 4), ")"),
hjust = -0.1, vjust = 1.5, color = "red", size = 4) +
labs(title = "Distribution of LOOCV Classification Errors Across All Models",
x = "Cross-Validated Classification Errors",
y = "Frequency") +
theme_minimal() +
theme(plot.title = element_text(hjust = 0.5, size = 14, face = "bold"))
model_results
print(head(model_results, 10))
model <- glm(price_class ~ sq_mt_built,
data = trimmed_df,
family = binomial)
model.prob <- predict(model, trimmed_df, type = "response")
# Convert probabilities to binary predictions using 0.5 threshold
predicted_class <- ifelse(model.prob >= 0.5, 1, 0)
actual <- test$price_class
# Calculate accuracy as (TP + TN) / Total
TP <- sum(predicted_class == 1 & actual == 1)
TN <- sum(predicted_class == 0 & actual == 0)
FP <- sum(predicted_class == 1 & actual == 0)
FN <- sum(predicted_class == 0 & actual == 1)
test_accuracy <- (TP + TN) / length(actual)
# Fit the best model
model <- glm(price_class ~ sq_mt_built,
data = trimmed_df,
family = binomial)
model <- glm(price_class ~ sq_mt_built,
data = trimmed_df,
family = binomial(link="logit"))
boxplot(sq_mt_built ~ price_class, data = trimmed_df,
main = "Property Size by Price Class",
xlab = "Price Class", ylab = "Square Meters")
# Check for extreme predictions
summary(fitted(model))
model.prob <- predict(model, trimmed_df, type = "response")
# Convert probabilities to binary predictions using 0.5 threshold
predicted_class <- ifelse(model.prob >= 0.5, 1, 0)
actual <- test$price_class
# Calculate accuracy as (TP + TN) / Total
TP <- sum(predicted_class == 1 & actual == 1)
TN <- sum(predicted_class == 0 & actual == 0)
FP <- sum(predicted_class == 1 & actual == 0)
FN <- sum(predicted_class == 0 & actual == 1)
test_accuracy <- (TP + TN) / length(actual)
# Print results
cat("\n=================================\n")
cat("Test Set Performance:\n")
cat("=================================\n")
cat("Test Accuracy:", round(test_accuracy, 4), "\n")
cat("Confusion Matrix:\n")
cat("  TP:", TP, "  TN:", TN, "\n")
cat("  FP:", FP, "  FN:", FN, "\n")
# Summary of the model
cat("\n=================================\n")
cat("Model Summary:\n")
cat("=================================\n")
summary(model)
trimmed_df
# Predict on test set
model.prob <- predict(model, test, type = "response")
# Fit the best model
model <- glm(price_class ~ sq_mt_built,
data = trimmed_df,
family = binomial(link="logit"))
# Predict on test set
model.prob <- predict(model, trimmed_df, type = "response")
# Convert probabilities to binary predictions using 0.5 threshold
predicted_class <- ifelse(model.prob >= 0.5, 1, 0)
actual <- trimmed_df$price_class
# Calculate accuracy as (TP + TN) / Total
TP <- sum(predicted_class == 1 & actual == 1)
TN <- sum(predicted_class == 0 & actual == 0)
FP <- sum(predicted_class == 1 & actual == 0)
FN <- sum(predicted_class == 0 & actual == 1)
test_accuracy <- (TP + TN) / length(actual)
# Print results
cat("\n=================================\n")
cat("Test Set Performance:\n")
cat("=================================\n")
cat("Test Accuracy:", round(test_accuracy, 4), "\n")
cat("Confusion Matrix:\n")
cat("  TP:", TP, "  TN:", TN, "\n")
cat("  FP:", FP, "  FN:", FN, "\n")
# Summary of the model
cat("\n=================================\n")
cat("Model Summary:\n")
cat("=================================\n")
summary(model)
double_loocv <- function(data, all_models, threshold = 0.5) {
n <- nrow(data)
outer_predictions <- numeric(n)
outer_actuals <- data$price_class
selected_models <- character(n)
cat("Starting Double LOOCV with", n, "outer iterations...\n\n")
# OUTER LOOP: Leave one observation out
for (i in 1:n) {
if (i %% 100 == 0) cat("Outer iteration", i, "of", n, "\n")
# Split data
outer_train <- data[-i, ]
outer_test <- data[i, ]
# INNER LOOP: Model selection using LOOCV on outer_train
inner_results <- data.frame(
model_id = integer(),
accuracy = numeric(),
stringsAsFactors = FALSE
)
for (j in 1:length(all_models)) {
predictors <- all_models[[j]]
# Perform LOOCV on outer_train for this model
inner_accuracy <- manual_loocv(predictors, outer_train, threshold)$accuracy
inner_results[j, "model_id"] <- j
inner_results[j, "accuracy"] <- inner_accuracy
}
# Select best model from inner LOOCV (highest accuracy)
best_model_idx <- which.max(inner_results$accuracy)
best_predictors <- all_models[[best_model_idx]]
selected_models[i] <- paste(best_predictors, collapse = ", ")
# Refit best model on full outer_train
formula_str <- paste("price_class ~", paste(best_predictors, collapse = " + "))
final_model <- glm(as.formula(formula_str),
data = outer_train,
family = binomial(link = "logit"))
# Predict on outer_test (the single left-out observation)
outer_predictions[i] <- predict(final_model, newdata = outer_test, type = "response")
}
# Convert probabilities to binary predictions
predicted_class <- ifelse(outer_predictions >= threshold, 1, 0)
# Calculate confusion matrix components
TP <- sum(predicted_class == 1 & outer_actuals == 1)
TN <- sum(predicted_class == 0 & outer_actuals == 0)
FP <- sum(predicted_class == 1 & outer_actuals == 0)
FN <- sum(predicted_class == 0 & outer_actuals == 1)
# Calculate final accuracy
final_accuracy <- (TP + TN) / n
return(list(
accuracy = final_accuracy,
cv_error = 1 - final_accuracy,
predictions = outer_predictions,
predicted_class = predicted_class,
selected_models = selected_models,
confusion = c(TP = TP, TN = TN, FP = FP, FN = FN)
))
}
# Run double LOOCV
cat("\n===========================================\n")
cat("Running Double/Nested LOOCV...\n")
cat("===========================================\n")
double_loocv_results <- double_loocv(trimmed_df, all_models)
cat("\n===========================================\n")
cat("Double LOOCV Results:\n")
cat("===========================================\n")
cat("Final Accuracy:", round(double_loocv_results$accuracy, 4), "\n")
cat("Final CV Error:", round(double_loocv_results$cv_error, 4), "\n")
cat("\nConfusion Matrix:\n")
print(double_loocv_results$confusion)
# See which models were most frequently selected
cat("\n===========================================\n")
cat("Most frequently selected models:\n")
cat("===========================================\n")
print(head(sort(table(double_loocv_results$selected_models), decreasing = TRUE), 10))
double_loocv_results
double_loocv_summary <- data.frame(
Metric = c("Accuracy", "Classification Error",
"True Positive (TP)", "True Negative (TN)",
"False Positive (FP)", "False Negative (FN)",
"Total Observations"),
Value = c(
round(double_loocv_results$accuracy, 4),
round(double_loocv_results$cv_error, 4),
double_loocv_results$confusion["TP"],
double_loocv_results$confusion["TN"],
double_loocv_results$confusion["FP"],
double_loocv_results$confusion["FN"],
length(double_loocv_results$predictions)
)
)
# Display results
cat("\n===========================================\n")
cat("Double LOOCV Results Summary:\n")
cat("===========================================\n")
print(double_loocv_summary, row.names = FALSE)
# Model selection frequency table
cat("\n===========================================\n")
cat("Model Selection Frequency:\n")
cat("===========================================\n")
model_freq <- sort(table(double_loocv_results$selected_models), decreasing = TRUE)
model_freq_df <- data.frame(
Model = names(model_freq),
Frequency = as.numeric(model_freq),
Percentage = round(100 * as.numeric(model_freq) / sum(model_freq), 2)
)
print(head(model_freq_df, 10), row.names = FALSE)
# Create summary table
double_loocv_summary <- data.frame(
Metric = c("Accuracy", "Classification Error",
"True Positive (TP)", "True Negative (TN)",
"False Positive (FP)", "False Negative (FN)",
"Total Observations"),
Value = c(
round(double_loocv_results$accuracy, 4),
round(double_loocv_results$cv_error, 4),
double_loocv_results$confusion["TP"],
double_loocv_results$confusion["TN"],
double_loocv_results$confusion["FP"],
double_loocv_results$confusion["FN"],
length(double_loocv_results$predictions)
)
)
# Display results
cat("\n===========================================\n")
cat("Double LOOCV Results Summary:\n")
cat("===========================================\n")
print(double_loocv_summary, row.names = FALSE)
# Model selection frequency table WITH accuracy and error
cat("\n===========================================\n")
cat("Model Selection Frequency:\n")
cat("===========================================\n")
model_freq <- sort(table(double_loocv_results$selected_models), decreasing = TRUE)
# Calculate accuracy for each unique model
unique_models <- names(model_freq)
model_accuracies <- numeric(length(unique_models))
for (i in 1:length(unique_models)) {
model_name <- unique_models[i]
# Find this model in the original results
model_match <- which(model_results$predictors == model_name)
if (length(model_match) > 0) {
model_accuracies[i] <- model_results$accuracy[model_match[1]]
} else {
model_accuracies[i] <- NA
}
}
model_freq_df <- data.frame(
Model = unique_models,
Frequency = as.numeric(model_freq),
Percentage = round(100 * as.numeric(model_freq) / sum(model_freq), 2),
Accuracy = round(model_accuracies, 4),
Error = round(1 - model_accuracies, 4)
)
print(head(model_freq_df, 10), row.names = FALSE)
model_freq_df
model_freq_df
